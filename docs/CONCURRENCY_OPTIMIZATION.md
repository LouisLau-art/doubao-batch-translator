# 快慢双车道并发优化策略

## 🎯 优化目标
充分利用不同模型的性能上限，实现智能分流：

### 慢车道：doubao-seed-translation-250915
- **RPM (每分钟请求数)**: 5,000 ≈ 83 req/sec
- **TPM (每分钟Token数)**: 500,000
- **并发限制**: 80
- **适用场景**: 免费额度使用、稳定翻译

### 快车道：其他高性能模型 (DeepSeek, Doubao Pro等)
- **RPM (每分钟请求数)**: 30,000 ≈ 500 req/sec
- **TPM (每分钟Token数)**: 通常更高
- **并发限制**: 500
- **适用场景**: 大量文本翻译、高并发需求

## 📊 优化前后对比

### 之前的单车道策略
| 参数 | 值 | 问题 |
|------|-----|------|
| 所有模型并发 | 80 | 高性能模型性能浪费 |
| seed-translation | 80 | ✅ 合理 |
| DeepSeek/Doubao Pro | 80 | ❌ 远低于500的上限 |

### 现在的双车道策略
| 模型类型 | 并发数 | RPM限制 | 利用率 |
|---------|--------|---------|--------|
| seed-translation | 80 | 5,000 | 96% ✅ |
| DeepSeek/Doubao Pro | 500 | 30,000 | 100% ✅ |

**性能提升**: 快车道模型吞吐量提升 **525%** (从80 → 500)

## 🔧 实现细节

### 1. Client层智能分流 (`core/client.py`)

```python
class AsyncDoubaoClient:
    def __init__(self, ...):
        # 双车道设计
        self.sem_fast = asyncio.Semaphore(500)  # 快车道：500并发
        self.sem_seed = asyncio.Semaphore(80)   # 慢车道：80并发
        
    def _get_semaphore(self, model: str):
        # 根据模型名称自动选择车道
        if "seed-translation" in model.lower():
            return self.sem_seed  # 慢车道
        return self.sem_fast      # 快车道
```

**关键优势**: 
- 完全自动化，用户无感知
- 每个请求根据模型动态选择车道
- 避免快慢模型互相影响

### 2. Server层统一限流 (`server/api.py`)

```python
class DoubaoServer:
    def __init__(self, config):
        # Server层设置为快车道上限，保护后端
        self.request_semaphore = asyncio.Semaphore(500)
```

**分层保护**:
- Server层: 500并发（防止过载）
- Client层: 自动选择80或500（精细控制）

### 3. 配置层灵活调整 (`core/config.py`)

```python
# 默认值已优化为快车道
DEFAULT_MAX_CONCURRENT = 500
DEFAULT_MAX_REQUESTS_PER_SECOND = 500.0
```

可通过环境变量覆盖：
```bash
MAX_CONCURRENT_REQUESTS=500
REQUESTS_PER_MINUTE=30000
```

## 📈 性能分析

### 吞吐量对比

| 场景 | 慢车道 (seed) | 快车道 (others) | 提升倍数 |
|------|--------------|----------------|----------|
| 并发数 | 80 | 500 | 6.25x |
| 理论RPS | 83 | 500 | 6.0x |
| 理论RPM | 5,000 | 30,000 | 6.0x |

### 实际场景测试（预期）

#### 场景1: 短文本翻译（沉浸式翻译）
- **模型**: seed-translation
- **平均Token**: 100 (输入) + 100 (输出) = 200
- **理论峰值**: 83 req/sec
- **瓶颈**: RPM

#### 场景2: 长文本翻译（文档批处理）
- **模型**: DeepSeek-v3
- **平均Token**: 1000 (输入) + 1000 (输出) = 2000
- **理论峰值**: 取决于TPM限制
- **瓶颈**: TPM

#### 场景3: 混合负载（推荐模式）
- **策略**: models.json中同时配置快慢车道
- **自动切换**: 长文本→快车道，短文本→慢车道
- **成本优化**: 充分利用免费额度

## 🚀 支持的模型列表

### 快车道模型 (500并发)
```json
[
  "deepseek-v3-250324",
  "deepseek-v3-1-terminus",
  "doubao-seed-1-6-lite-251015",
  "doubao-seed-1-6-251015",
  "doubao-1-5-vision-pro-32k-250115",
  "doubao-1.5-vision-pro-250328",
  "doubao-1.5-vision-lite-250315",
  "doubao-1-5-ui-tars-250428"
]
```

### 慢车道模型 (80并发)
```json
[
  "doubao-seed-translation-250915"
]
```

## 💡 使用建议

### 推荐配置 (models.json)

```json
[
  "doubao-seed-translation-250915",  // 优先使用免费额度
  "deepseek-v3-250324",              // 备用快车道
  "doubao-seed-1-6-251015"           // 备用快车道
]
```

**工作原理**:
1. 优先使用 seed-translation (慢车道，免费)
2. 如果失败或超额，自动切换到快车道模型
3. 每个模型使用对应的并发限制

### 性能测试建议

#### 测试慢车道
```bash
python test_concurrency.py
# 会看到 seed-translation 并发限制在80左右
```

#### 测试快车道
修改 `models.json`，将快车道模型放在首位：
```json
["deepseek-v3-250324", ...]
```
然后运行测试，会看到并发可达500

## ⚠️ 注意事项

1. **慢车道触发限流**
   - 症状: 429错误，错误信息包含 "SetLimitExceeded"
   - 原因: 超过RPM=5000限制
   - 解决: 正常现象，系统会自动切换到快车道

2. **快车道成本**
   - 快车道模型通常按调用量计费
   - 建议在 models.json 中先配置慢车道，作为首选
   - 只在必要时使用快车道

3. **混合使用优化**
   - 系统支持在单次运行中混用快慢车道
   - 客户端会根据模型名称自动选择并发限制
   - 无需手动干预

## 📊 监控建议

### 关键指标
1. **并发使用率**: 慢车道应接近80，快车道可达500
2. **API错误率**: < 1% 为正常
3. **平均响应时间**: 
   - 慢车道: 0.5-2秒
   - 快车道: 0.3-1.5秒
4. **Token使用量**: 确保不超过TPM限制

### 告警阈值
- 429错误率 > 5%: 降低并发
- 平均响应时间 > 5秒: 检查网络或降低并发
- Token使用超过90%: 注意额度，考虑限流

## 🎓 架构优势

### 1. 自动化
- ✅ 无需手动配置快慢车道
- ✅ 根据模型名称自动识别
- ✅ 透明切换，用户无感知

### 2. 高性能
- ✅ 慢车道: 96%利用率 (80/83)
- ✅ 快车道: 100%利用率 (500/500)
- ✅ 整体吞吐量提升6倍

### 3. 成本优化
- ✅ 优先使用免费慢车道
- ✅ 失败时自动切换快车道
- ✅ 智能负载均衡

### 4. 可扩展性
- ✅ 新增模型只需添加到models.json
- ✅ 自动识别并选择合适车道
- ✅ 支持动态配置

## 🔮 未来优化方向

1. **动态限流**: 根据实时API错误率自动调整并发
2. **智能降级**: 慢车道超限时自动切换快车道
3. **成本监控**: 实时追踪API调用成本
4. **性能分析**: 记录每个模型的响应时间和成功率

---

**总结**: 快慢双车道策略实现了性能与成本的最优平衡，慢车道充分利用免费额度，快车道满足高并发需求，两者自动协作，无缝切换。
