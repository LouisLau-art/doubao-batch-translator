#!/usr/bin/env python3
"""
è±†åŒ…ç¿»è¯‘æ¨¡å‹ç»Ÿä¸€æ¥å£ - ä¸»å…¥å£
æ”¯æŒCLIå‘½ä»¤è¡Œå·¥å…·ã€HTTP ServeræœåŠ¡ã€ä»¥åŠæ™ºèƒ½ ePub æ¼è¯‘ä¿®å¤é—­ç¯
"""

import argparse
import asyncio
import logging
import sys
import os
import tempfile
import zipfile
import shutil
from pathlib import Path
from typing import Optional, Dict, List

# ç¡®ä¿èƒ½æ‰¾åˆ° core æ¨¡å—
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from core.config import TranslatorConfig
from core.client import AsyncTranslator
from processors.json_worker import JSONProcessor
from processors.html_worker import HTMLProcessor
from processors.epub_worker import EpubProcessor
from server.api import run_server

# å°è¯•å¯¼å…¥è´¨æ£€å·¥å…·
try:
    from check_untranslated import EPUBTranslationChecker
except ImportError:
    EPUBTranslationChecker = None

# é…ç½®æ—¥å¿—
from logging.handlers import RotatingFileHandler

# æ—¥å¿—æ–‡ä»¶å›ºå®šåœ¨é¡¹ç›®ç›®å½•ä¸‹
_LOG_FILE = Path(__file__).parent / 'doubao-translator.log'

# åˆ›å»ºæ ¼å¼åŒ–å™¨ (åŒ…å«æ¨¡å—åä¾¿äºè°ƒè¯•)
_log_formatter = logging.Formatter(
    '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# æ§åˆ¶å°å¤„ç†å™¨ (ç®€æ´æ ¼å¼)
_console_handler = logging.StreamHandler(sys.stdout)
_console_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
_console_handler.setLevel(logging.INFO)

# æ–‡ä»¶å¤„ç†å™¨ (å¸¦è½®è½¬: 10MB, ä¿ç•™3ä»½)
_file_handler = RotatingFileHandler(
    _LOG_FILE, 
    maxBytes=10*1024*1024,  # 10MB
    backupCount=3,
    encoding='utf-8'
)
_file_handler.setFormatter(_log_formatter)
_file_handler.setLevel(logging.DEBUG)  # æ–‡ä»¶è®°å½•æ›´è¯¦ç»†

# é…ç½®æ ¹æ—¥å¿—
logging.basicConfig(level=logging.DEBUG, handlers=[_console_handler, _file_handler])

# é™ä½ç¬¬ä¸‰æ–¹åº“æ—¥å¿—çº§åˆ«
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)

logger = logging.getLogger(__name__)


class MainCLI:
    """ä¸»å‘½ä»¤è¡Œç•Œé¢"""
    
    def __init__(self):
        self.parser = self._create_parser()
    
    def _create_parser(self) -> argparse.ArgumentParser:
        """åˆ›å»ºå‚æ•°è§£æå™¨"""
        parser = argparse.ArgumentParser(
            description="è±†åŒ…ç¿»è¯‘æ¨¡å‹ç»Ÿä¸€æ¥å£ - æ™ºèƒ½é—­ç¯ç‰ˆ",
            formatter_class=argparse.RawDescriptionHelpFormatter
        )
        
        # é€šç”¨å‚æ•°
        parser.add_argument("--api-key", help="è±†åŒ…APIå¯†é’¥")
        parser.add_argument("--verbose", "-v", action="store_true", help="å¯ç”¨è¯¦ç»†æ—¥å¿—")
        parser.add_argument("--max-concurrent", type=int, help="æœ€å¤§å¹¶å‘è¯·æ±‚æ•° (å»ºè®®: 30-100)")
        parser.add_argument("--max-rps", type=float, help="æ¯ç§’æœ€å¤§è¯·æ±‚æ•° (å»ºè®®: 20.0)")
        
        # å­å‘½ä»¤
        subparsers = parser.add_subparsers(dest="command", help="å¯ç”¨å‘½ä»¤")
        
        # JSONç¿»è¯‘å‘½ä»¤
        json_parser = subparsers.add_parser("json", help="JSONæ–‡ä»¶ç¿»è¯‘")
        json_parser.add_argument("--file", "-f", required=True, help="è¾“å…¥æ–‡ä»¶")
        json_parser.add_argument("--output", "-o", help="è¾“å‡ºæ–‡ä»¶")
        json_parser.add_argument("--source-lang", help="æºè¯­è¨€")
        json_parser.add_argument("--target-lang", "-t", default="zh", help="ç›®æ ‡è¯­è¨€")
        
        # HTMLç¿»è¯‘å‘½ä»¤
        html_parser = subparsers.add_parser("html", help="HTMLæ–‡ä»¶ç¿»è¯‘")
        html_parser.add_argument("--file", "-f", required=True, help="è¾“å…¥æ–‡ä»¶")
        html_parser.add_argument("--output", "-o", help="è¾“å‡ºæ–‡ä»¶")
        html_parser.add_argument("--source-lang", help="æºè¯­è¨€")
        html_parser.add_argument("--target-lang", "-t", default="zh", help="ç›®æ ‡è¯­è¨€")
        
        # ePubç¿»è¯‘å‘½ä»¤
        epub_parser = subparsers.add_parser("epub", help="ePubç”µå­ä¹¦ç¿»è¯‘")
        epub_parser.add_argument("--file", "-f", required=True, help="è¾“å…¥æ–‡ä»¶ æˆ– æ–‡ä»¶å¤¹")
        epub_parser.add_argument("--output", "-o", help="è¾“å‡ºæ–‡ä»¶ æˆ– è¾“å‡ºæ–‡ä»¶å¤¹")
        epub_parser.add_argument("--source-lang", help="æºè¯­è¨€")
        epub_parser.add_argument("--target-lang", "-t", default="zh", help="ç›®æ ‡è¯­è¨€")
        # [ä¿®æ”¹] å°† auto-approve ç§»åˆ°è¿™é‡Œï¼Œä½œä¸º epub å­å‘½ä»¤çš„å‚æ•°
        epub_parser.add_argument("--auto-approve", action="store_true", help="è‡ªåŠ¨åŒæ„è´¨æ£€ä¿®å¤ï¼Œæ— éœ€äººå·¥ç¡®è®¤")
        
        # Serverå‘½ä»¤
        server_parser = subparsers.add_parser("server", help="å¯åŠ¨HTTP APIæœåŠ¡å™¨")
        server_parser.add_argument("--host", default="0.0.0.0", help="ç»‘å®šåœ°å€")
        server_parser.add_argument("--port", "-p", type=int, default=8000, help="ç›‘å¬ç«¯å£")
        server_parser.add_argument("--debug", action="store_true", help="å¯ç”¨è°ƒè¯•æ¨¡å¼")
        
        # [æ–°å¢] äººå·¥ç¿»è¯‘å›å¡«å‘½ä»¤
        applyfix_parser = subparsers.add_parser("apply-fix", help="å°†äººå·¥ç¿»è¯‘çš„JSONå›å¡«åˆ°ePub")
        applyfix_parser.add_argument("--json", "-j", required=True, help="äººå·¥ç¿»è¯‘.json æ–‡ä»¶è·¯å¾„")
        
        # [æ–°å¢] é‡æ–°ç”Ÿæˆæ¼è¯‘ JSON (é’ˆå¯¹å·²ç¿»è¯‘çš„ EPUB)
        genjson_parser = subparsers.add_parser("generate-json", help="æ‰«æå·²ç¿»è¯‘EPUBå¹¶ç”Ÿæˆäººå·¥ç¿»è¯‘JSON")
        genjson_parser.add_argument("--dir", "-d", required=True, help="å·²ç¿»è¯‘EPUBæ‰€åœ¨ç›®å½•")
        
        return parser
    
    def _get_config(self, args) -> TranslatorConfig:
        """è·å–é…ç½®å¯¹è±¡"""
        config_kwargs = {}
        if hasattr(args, 'max_concurrent') and args.max_concurrent:
            config_kwargs['max_concurrent'] = args.max_concurrent
        if hasattr(args, 'max_rps') and args.max_rps:
            config_kwargs['max_requests_per_second'] = args.max_rps
            
        try:
            return TranslatorConfig.from_args(
                api_key=args.api_key, 
                **config_kwargs
            )
        except Exception as e:
            logger.error(f"é…ç½®åŠ è½½å¤±è´¥: {e}")
            sys.exit(1)
    
    def _create_translator(self, config: TranslatorConfig) -> AsyncTranslator:
        """å·¥å‚æ–¹æ³•"""
        return AsyncTranslator(config)

    def _print_stats(self, translator: AsyncTranslator):
        """æ‰“å°æ¨¡å‹ä½¿ç”¨ç»Ÿè®¡ (å« Token)"""
        if not hasattr(translator, 'get_stats'):
            return

        stats = translator.get_stats()
        total_requests = 0
        total_in = 0
        total_out = 0
        
        for data in stats.values():
            total_requests += data.get('calls', 0)
            total_in += data.get('input', 0)
            total_out += data.get('output', 0)
        
        if total_requests == 0:
            return

        print("\n" + "="*85)
        print("ğŸ“Š æ¨¡å‹ä½¿ç”¨ç»Ÿè®¡æŠ¥å‘Š")
        print("="*85)
        print(f"{'æ¨¡å‹åç§°':<35} | {'æ¬¡æ•°':<6} | {'å æ¯”':<6} | {'Input Tokens':<12} | {'Output Tokens':<12}")
        print("-" * 85)
        
        sorted_stats = sorted(stats.items(), key=lambda x: x[1]['calls'], reverse=True)
        
        for model, data in sorted_stats:
            count = data.get('calls', 0)
            if count > 0:
                percentage = (count / total_requests) * 100
                in_t = data.get('input', 0)
                out_t = data.get('output', 0)
                print(f"{model:<35} | {count:<6} | {percentage:.1f}%  | {in_t:<12,} | {out_t:<12,}")
        
        print("-" * 85)
        print(f"{'æ€»è®¡':<35} | {total_requests:<6} | 100%   | {total_in:<12,} | {total_out:<12,}")
        print("="*85 + "\n")

    def _repack_epub(self, source_dir: str, output_path: str):
        """é‡æ–°æ‰“åŒ… ePub å·¥å…·å‡½æ•°"""
        with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zf:
            mimetype_path = os.path.join(source_dir, 'mimetype')
            if os.path.exists(mimetype_path):
                zf.write(mimetype_path, 'mimetype', compress_type=zipfile.ZIP_STORED)
            for root, _, files in os.walk(source_dir):
                for f in files:
                    if f == 'mimetype': continue
                    full_path = os.path.join(root, f)
                    arc_name = os.path.relpath(full_path, source_dir)
                    zf.write(full_path, arc_name)

    async def _run_interactive_patch_loop(self, epub_path: str, config: TranslatorConfig, target_lang: str, auto_approve: bool = False):
        """äº¤äº’å¼è´¨æ£€ä¸ä¿®å¤é—­ç¯"""
        if not EPUBTranslationChecker:
            logger.warning("æœªæ‰¾åˆ° check_untranslated.pyï¼Œè·³è¿‡è´¨æ£€ç¯èŠ‚ã€‚")
            return

        checker = EPUBTranslationChecker()
        round_count = 1
        MAX_PATCH_ROUNDS = 5  # [æ–°å¢] æœ€å¤§ä¿®å¤è½®æ¬¡ï¼Œé˜²æ­¢æ­»å¾ªç¯
        
        while True:
            # [æ–°å¢] æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§è½®æ¬¡
            if round_count > MAX_PATCH_ROUNDS:
                print(f"\nâš ï¸  å·²è¾¾åˆ°æœ€å¤§ä¿®å¤è½®æ¬¡ ({MAX_PATCH_ROUNDS})ï¼Œåœæ­¢è‡ªåŠ¨ä¿®å¤ã€‚")
                print("ğŸ’¡ å‰©ä½™æ¼è¯‘å¯èƒ½æ˜¯è¯¯åˆ¤æˆ–éœ€è¦äººå·¥å¤„ç†ã€‚")
                break
            
            print(f"\nğŸ” [ç¬¬ {round_count} è½®è´¨æ£€] æ­£åœ¨æ‰«ææ¼è¯‘æ®µè½...")
            try:
                # æ•è·æ£€æŸ¥è¿‡ç¨‹ä¸­çš„ printï¼Œåªæ˜¾ç¤ºç»“æœ
                report = checker.check_epub(epub_path)
                untranslated_count = report['untranslated_count']
                
                if untranslated_count == 0:
                    print("\nğŸ‰ å®Œç¾ï¼æ£€æµ‹ç»“æœæ˜¾ç¤ºæ²¡æœ‰æ¼è¯‘ã€‚")
                    break
                
                print(f"\nâš ï¸  å‘ç° {untranslated_count} å¤„ç–‘ä¼¼æ¼è¯‘ï¼Œæ¶‰åŠ {len(set(i['file'] for i in report['details']))} ä¸ªæ–‡ä»¶")
                
                if auto_approve:
                    print("ğŸ¤– [è‡ªåŠ¨æ¨¡å¼] å·²æ£€æµ‹åˆ°æ¼è¯‘ï¼Œè‡ªåŠ¨å¼€å§‹ä¿®å¤...")
                    choice = 'y'
                else:
                    # æ‰“å°å‰3ä¸ªç¤ºä¾‹
                    for i, item in enumerate(report['details'][:3]):
                        print(f"   - [{item['tag']}] {item['text'][:60]}...")
                    if len(report['details']) > 3:
                        print(f"   ... ç­‰å…± {untranslated_count} å¤„")
                    choice = input("\nğŸ‘‰ æ˜¯å¦å¯¹è¿™äº›æ¼è¯‘è¿›è¡Œã€å¼ºåˆ¶ä¿®å¤ã€‘(y/n)? [é»˜è®¤ä¸ºy]: ").strip().lower()
                
                if choice == 'n':
                    print("ç”¨æˆ·é€‰æ‹©ç»“æŸæµç¨‹ã€‚")
                    break
                
                # å¼€å§‹ä¿®å¤
                print(f"\nğŸ’‰ [ä¿®å¤æ¨¡å¼] æ­£åœ¨å¯åŠ¨...")
                files_to_fix = set(item['file'] for item in report['details'])
                
                # [ä¿®å¤] ä½¿ç”¨ dataclasses.replace åˆ›å»ºå‰¯æœ¬ï¼Œé¿å…æ±¡æŸ“åŸå§‹ config
                from dataclasses import replace
                patch_config = replace(config, max_concurrent=50)
                
                async with self._create_translator(patch_config) as patch_translator:
                    patch_processor = HTMLProcessor(patch_translator)
                    
                    # [ä¿®å¤] ä¸´æ—¶ç¦ç”¨è¿‡æ»¤å™¨ (Monkey Patch) - åŒ…æ‹¬ URL è¿‡æ»¤å’Œä¸­æ–‡æ£€æµ‹
                    original_url_filter = patch_processor._is_url_or_code
                    original_chinese_filter = patch_processor._is_chinese_text
                    patch_processor._is_url_or_code = lambda text: False 
                    patch_processor._is_chinese_text = lambda text: False  # [æ–°å¢] å¼ºåˆ¶ç¿»è¯‘æ‰€æœ‰å†…å®¹
                    
                    with tempfile.TemporaryDirectory() as temp_dir:
                        # 1. è§£å‹
                        with zipfile.ZipFile(epub_path, 'r') as zf:
                            zf.extractall(temp_dir)
                        
                        # 2. ä¿®å¤ (å¸¦è¿›åº¦æ¡)
                        total_files = len(files_to_fix)
                        completed = [0]  # ä½¿ç”¨åˆ—è¡¨ä»¥ä¾¿åœ¨é—­åŒ…ä¸­ä¿®æ”¹
                        
                        async def process_with_progress(rel_path: str):
                            full_path = os.path.join(temp_dir, rel_path)
                            if os.path.exists(full_path):
                                logger.info(f"   æ­£åœ¨ä¿®è¡¥: {rel_path}")
                                await patch_processor.process_file(full_path, full_path, target_lang=target_lang)
                            completed[0] += 1
                            # å®æ—¶è¿›åº¦æ¡
                            progress = completed[0] / total_files
                            bar_length = 30
                            block = int(round(bar_length * progress))
                            sys.stdout.write(f"\rä¿®å¤è¿›åº¦: [{'#' * block}{'-' * (bar_length - block)}] {progress * 100:.1f}% ({completed[0]}/{total_files})")
                            sys.stdout.flush()
                        
                        tasks = [process_with_progress(rel_path) for rel_path in files_to_fix]
                        await asyncio.gather(*tasks)
                        print()  # è¿›åº¦æ¡ç»“æŸåæ¢è¡Œ
                        
                        # 3. æ‰“åŒ…
                        self._repack_epub(temp_dir, epub_path)
                    
                    # æ¢å¤è¿‡æ»¤å™¨
                    patch_processor._is_url_or_code = original_url_filter
                    patch_processor._is_chinese_text = original_chinese_filter
                    
                    print(f"âœ… ä¿®å¤å®Œæˆï¼Œå·²æ›´æ–°æ–‡ä»¶: {epub_path}")
                    self._print_stats(patch_translator)
                
                round_count += 1
                
            except Exception as e:
                logger.error(f"è´¨æ£€å¾ªç¯å‘ç”Ÿé”™è¯¯: {e}")
                import traceback
                traceback.print_exc()
                break

    async def _process_single_epub(self, input_path: str, output_path: str, config: TranslatorConfig, args):
        """å¤„ç†å•æœ¬ ePub çš„æ ¸å¿ƒé€»è¾‘ (åŒ…å«ç¿»è¯‘+è´¨æ£€)"""
        print(f"\nğŸ“˜ æ­£åœ¨å¤„ç†: {os.path.basename(input_path)}")
        print(f"   è¾“å‡ºè‡³: {output_path}")
        
        # 1. æ£€æŸ¥æ˜¯å¦å­˜åœ¨ï¼Œå†³å®šæ˜¯å¦è·³è¿‡ç¬¬ä¸€é˜¶æ®µ
        skip_main = False
        if os.path.exists(output_path):
            if args.auto_approve:
                print(f"â© è¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨ï¼Œè‡ªåŠ¨è·³è¿‡å…¨é‡ç¿»è¯‘ï¼Œè¿›å…¥è´¨æ£€...")
                skip_main = True
            else:
                choice = input(f"\nğŸ“‚ è¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨ã€‚\nğŸ‘‰ æ˜¯å¦è·³è¿‡å…¨é‡ç¿»è¯‘ï¼Œç›´æ¥è¿›å…¥ã€è´¨æ£€ä¸ä¿®å¤ã€‘? (y/n): ").strip().lower()
                if choice == 'y':
                    skip_main = True

        # 2. å…¨é‡ç¿»è¯‘é˜¶æ®µ
        if not skip_main:
            def progress_callback(progress: float, message: str):
                bar_length = 30
                block = int(round(bar_length * progress))
                sys.stdout.write(f"\rè¿›åº¦: [{'#' * block}{'-' * (bar_length - block)}] {progress * 100:.1f}% - {message}")
                sys.stdout.flush()

            async with self._create_translator(config) as translator:
                processor = EpubProcessor(translator)
                try:
                    await processor.translate_epub(
                        input_path=input_path,
                        output_path=output_path,
                        source_lang=args.source_lang,
                        target_lang=args.target_lang,
                        progress_callback=progress_callback
                    )
                    print("\n")
                    self._print_stats(translator)
                except Exception as e:
                    print("\n")
                    logger.error(f"ePubç¿»è¯‘ä¸­æ–­: {e}")
                    self._print_stats(translator)
                    if not os.path.exists(output_path):
                        return

        # 3. è´¨æ£€ä¸ä¿®å¤é˜¶æ®µ
        if os.path.exists(output_path):
            await self._run_interactive_patch_loop(output_path, config, args.target_lang, auto_approve=args.auto_approve)

    async def _handle_epub_command(self, args):
        config = self._get_config(args)
        
        if config.models:
            print(f"ğŸš€ æ¨¡å‹æ± å·²åŠ è½½: {len(config.models)} ä¸ªæ¨¡å‹")
            print(f"   é¦–é€‰: {config.models[0]}")

        input_path = Path(args.file)
        
        # --- åœºæ™¯ A: å•æ–‡ä»¶ ---
        if input_path.is_file():
            # ç¡®å®šè¾“å‡ºè·¯å¾„
            if args.output:
                out_p = Path(args.output)
                if out_p.is_dir():
                    output_path = out_p / f"{input_path.stem}_translated{input_path.suffix}"
                else:
                    output_path = out_p
            else:
                output_path = input_path.with_name(f"{input_path.stem}_translated{input_path.suffix}")
            
            await self._process_single_epub(str(input_path), str(output_path), config, args)

        # --- åœºæ™¯ B: æ–‡ä»¶å¤¹ (æ‰¹é‡) - æ–°æµç¨‹ ---
        elif input_path.is_dir():
            epub_files = list(input_path.glob("*.epub"))
            # æ’é™¤å·²ç¿»è¯‘æ–‡ä»¶
            epub_files = [f for f in epub_files if "_translated" not in f.name and "é—´å¥æ›²" not in f.name]
            
            if not epub_files:
                logger.error(f"åœ¨ {input_path} ä¸­æœªæ‰¾åˆ° .epub æ–‡ä»¶")
                return

            print(f"\nğŸ“š å‘ç° {len(epub_files)} æœ¬ç”µå­ä¹¦")
            print("=" * 60)
            print("ğŸ“‹ æ‰¹é‡å¤„ç†æµç¨‹:")
            print("   é˜¶æ®µ1: å…¨é‡ç¿»è¯‘æ‰€æœ‰ä¹¦ç±")
            print("   é˜¶æ®µ2: ç»Ÿä¸€è´¨æ£€ä¸ä¿®å¤")
            print("   é˜¶æ®µ3: ç”Ÿæˆæ¼è¯‘æŠ¥å‘Š")
            print("=" * 60)
            
            if args.output:
                output_dir = Path(args.output)
                output_dir.mkdir(parents=True, exist_ok=True)
            else:
                output_dir = input_path 
            
            # ========== é˜¶æ®µ1: å…¨é‡ç¿»è¯‘ ==========
            print(f"\n{'='*60}")
            print("ğŸ“– [é˜¶æ®µ1] å…¨é‡ç¿»è¯‘")
            print(f"{'='*60}")
            
            translated_files = []  # æ”¶é›†å·²ç¿»è¯‘çš„æ–‡ä»¶
            
            for idx, file in enumerate(epub_files, 1):
                print(f"\nğŸ“¦ [{idx}/{len(epub_files)}] {file.name}")
                
                output_filename = f"{file.stem}_translated{file.suffix}"
                output_path = output_dir / output_filename
                
                try:
                    await self._translate_epub_only(str(file), str(output_path), config, args)
                    if output_path.exists():
                        translated_files.append(output_path)
                except Exception as e:
                    logger.error(f"ç¿»è¯‘ {file.name} å¤±è´¥: {e}")
                    continue
            
            print(f"\nâœ… é˜¶æ®µ1å®Œæˆ: {len(translated_files)}/{len(epub_files)} æœ¬ä¹¦å·²ç¿»è¯‘")
            
            # ========== é˜¶æ®µ2: ç»Ÿä¸€è´¨æ£€ä¸ä¿®å¤ ==========
            if translated_files and EPUBTranslationChecker:
                print(f"\n{'='*60}")
                print("ğŸ” [é˜¶æ®µ2] ç»Ÿä¸€è´¨æ£€ä¸ä¿®å¤")
                print(f"{'='*60}")
                
                final_report = await self._batch_patch_all(translated_files, config, args.target_lang)
                
                # ========== é˜¶æ®µ3: ç”ŸæˆæŠ¥å‘Š ==========
                if final_report:
                    self._generate_final_report(final_report, output_dir)
            else:
                print("\nâ­ï¸  è·³è¿‡è´¨æ£€é˜¶æ®µ (æ— å·²ç¿»è¯‘æ–‡ä»¶æˆ–ç¼ºå°‘è´¨æ£€å·¥å…·)")
                
        else:
            logger.error(f"è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {args.file}")
            sys.exit(1)
    
    async def _translate_epub_only(self, input_path: str, output_path: str, config: TranslatorConfig, args):
        """ä»…æ‰§è¡Œç¿»è¯‘ï¼Œä¸è¿›è¡Œè´¨æ£€ï¼ˆç”¨äºæ‰¹é‡å¤„ç†é˜¶æ®µ1ï¼‰"""
        print(f"   è¾“å‡ºè‡³: {output_path}")
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if os.path.exists(output_path):
            print(f"   â© è¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ç¿»è¯‘")
            return
        
        def progress_callback(progress: float, message: str):
            bar_length = 30
            block = int(round(bar_length * progress))
            sys.stdout.write(f"\r   è¿›åº¦: [{'#' * block}{'-' * (bar_length - block)}] {progress * 100:.1f}% - {message}")
            sys.stdout.flush()

        async with self._create_translator(config) as translator:
            processor = EpubProcessor(translator)
            try:
                await processor.translate_epub(
                    input_path=input_path,
                    output_path=output_path,
                    source_lang=args.source_lang,
                    target_lang=args.target_lang,
                    progress_callback=progress_callback
                )
                print("\n")
                self._print_stats(translator)
            except Exception as e:
                print("\n")
                logger.error(f"ePubç¿»è¯‘ä¸­æ–­: {e}")
                self._print_stats(translator)
    
    async def _batch_patch_all(self, epub_files: List[Path], config: TranslatorConfig, target_lang: str) -> Dict:
        """ç»Ÿä¸€å¯¹æ‰€æœ‰å·²ç¿»è¯‘æ–‡ä»¶è¿›è¡Œè´¨æ£€å’Œä¿®å¤"""
        MAX_PATCH_ROUNDS = 3  # æ‰¹é‡æ¨¡å¼ä¸‹å‡å°‘ä¿®å¤è½®æ¬¡
        checker = EPUBTranslationChecker()
        
        # æ”¶é›†æ‰€æœ‰æ¼è¯‘æŠ¥å‘Š
        all_reports = {}  # {æ–‡ä»¶è·¯å¾„: æœ€ç»ˆæ¼è¯‘è¯¦æƒ…}
        
        for round_count in range(1, MAX_PATCH_ROUNDS + 1):
            print(f"\nğŸ”„ [ä¿®å¤è½®æ¬¡ {round_count}/{MAX_PATCH_ROUNDS}]")
            
            files_need_fix = []  # æœ¬è½®éœ€è¦ä¿®å¤çš„æ–‡ä»¶
            
            # æ‰«ææ‰€æœ‰æ–‡ä»¶
            for epub_path in epub_files:
                try:
                    report = checker.check_epub(str(epub_path))
                    if report['untranslated_count'] > 0:
                        files_need_fix.append((epub_path, report))
                        print(f"   âš ï¸  {epub_path.name}: {report['untranslated_count']} å¤„æ¼è¯‘")
                    else:
                        print(f"   âœ… {epub_path.name}: æ— æ¼è¯‘")
                except Exception as e:
                    logger.warning(f"æ£€æŸ¥ {epub_path.name} å¤±è´¥: {e}")
            
            if not files_need_fix:
                print("\nğŸ‰ æ‰€æœ‰æ–‡ä»¶å‡æ— æ¼è¯‘!")
                break
            
            print(f"\n   ğŸ“ æœ¬è½®éœ€ä¿®å¤: {len(files_need_fix)} ä¸ªæ–‡ä»¶")
            
            # æ‰¹é‡ä¿®å¤
            from dataclasses import replace
            patch_config = replace(config, max_concurrent=50)
            
            async with self._create_translator(patch_config) as patch_translator:
                for epub_path, report in files_need_fix:
                    try:
                        await self._patch_single_epub(epub_path, report, patch_translator, target_lang)
                    except Exception as e:
                        logger.error(f"ä¿®å¤ {epub_path.name} å¤±è´¥: {e}")
                
                self._print_stats(patch_translator)
        
        # æœ€ç»ˆæ‰«æï¼Œæ”¶é›†å‰©ä½™æ¼è¯‘
        print(f"\nğŸ“‹ æœ€ç»ˆæ£€æŸ¥...")
        for epub_path in epub_files:
            try:
                report = checker.check_epub(str(epub_path))
                if report['untranslated_count'] > 0:
                    all_reports[str(epub_path)] = report['details']
                    print(f"   âš ï¸  {epub_path.name}: ä»æœ‰ {report['untranslated_count']} å¤„æ¼è¯‘")
                else:
                    print(f"   âœ… {epub_path.name}: å®Œç¾")
            except Exception as e:
                logger.warning(f"æœ€ç»ˆæ£€æŸ¥ {epub_path.name} å¤±è´¥: {e}")
        
        return all_reports
    
    async def _patch_single_epub(self, epub_path: Path, report: Dict, translator, target_lang: str):
        """ä¿®å¤å•ä¸ª epub æ–‡ä»¶çš„æ¼è¯‘"""
        files_to_fix = set(item['file'] for item in report['details'])
        
        patch_processor = HTMLProcessor(translator)
        # ç¦ç”¨è¿‡æ»¤å™¨
        patch_processor._is_url_or_code = lambda text: False
        patch_processor._is_chinese_text = lambda text: False
        
        with tempfile.TemporaryDirectory() as temp_dir:
            with zipfile.ZipFile(str(epub_path), 'r') as zf:
                zf.extractall(temp_dir)
            
            tasks = []
            for rel_path in files_to_fix:
                full_path = os.path.join(temp_dir, rel_path)
                if os.path.exists(full_path):
                    tasks.append(patch_processor.process_file(full_path, full_path, target_lang=target_lang))
            
            await asyncio.gather(*tasks)
            self._repack_epub(temp_dir, str(epub_path))
        
        print(f"   âœ… å·²ä¿®å¤: {epub_path.name}")
    
    def _generate_final_report(self, reports: Dict, output_dir: Path):
        """ç”Ÿæˆæœ€ç»ˆæ¼è¯‘æŠ¥å‘Š (å«äººå·¥ç¿»è¯‘ç”¨çš„ JSON)"""
        import json
        from datetime import datetime
        
        if not reports:
            print("\nğŸ‰ å¤ªæ£’äº†ï¼æ‰€æœ‰æ–‡ä»¶å‡å·²å®Œç¾ç¿»è¯‘ï¼Œæ— éœ€äººå·¥å¤„ç†ã€‚")
            return
        
        report_path = output_dir / "æ¼è¯‘æŠ¥å‘Š.txt"
        json_path = output_dir / "äººå·¥ç¿»è¯‘.json"
        
        total_issues = sum(len(details) for details in reports.values())
        
        # ========== 1. ç”Ÿæˆ TXT æŠ¥å‘Š (äººç±»å¯è¯») ==========
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write("ğŸ“‹ ePub æ‰¹é‡ç¿»è¯‘ - æ¼è¯‘æŠ¥å‘Š\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("=" * 60 + "\n\n")
            f.write(f"å…± {len(reports)} ä¸ªæ–‡ä»¶å­˜åœ¨æ¼è¯‘ï¼Œæ€»è®¡ {total_issues} å¤„\n\n")
            f.write("ğŸ’¡ äººå·¥ç¿»è¯‘æ–¹æ³•:\n")
            f.write("   1. æ‰“å¼€åŒç›®å½•ä¸‹çš„ 'äººå·¥ç¿»è¯‘.json'\n")
            f.write("   2. åœ¨æ¯ä¸ªæ¡ç›®çš„ 'translation' å­—æ®µå¡«å…¥æ‚¨çš„è¯‘æ–‡\n")
            f.write("   3. è¿è¡Œ: python3 main.py apply-fix --json äººå·¥ç¿»è¯‘.json\n")
            f.write("=" * 60 + "\n\n")
            
            for epub_path, details in reports.items():
                f.write("-" * 40 + "\n")
                f.write(f"ğŸ“– {Path(epub_path).name}\n")
                f.write(f"   æ¼è¯‘æ•°: {len(details)}\n\n")
                
                # æŒ‰æ–‡ä»¶åˆ†ç»„
                by_file = {}
                for item in details:
                    file_name = item['file']
                    if file_name not in by_file:
                        by_file[file_name] = []
                    by_file[file_name].append(item)
                
                for file_name, items in by_file.items():
                    f.write(f"   ğŸ“„ {file_name}:\n")
                    for item in items[:10]:
                        text_preview = item['text'][:80].replace('\n', ' ')
                        f.write(f"      [{item['tag']}] {text_preview}...\n")
                    if len(items) > 10:
                        f.write(f"      ... ç­‰ {len(items)} å¤„\n")
                    f.write("\n")
        
        # ========== 2. ç”Ÿæˆäººå·¥ç¿»è¯‘ç”¨çš„ JSON ==========
        # ç»“æ„è®¾è®¡: ä»¥ epub æ–‡ä»¶ä¸ºå•ä½ï¼Œæ¯ä¸ªæ¡ç›®åŒ…å«åŸæ–‡å’Œç©ºçš„è¯‘æ–‡å­—æ®µ
        json_data = {
            "_è¯´æ˜": "è¯·åœ¨æ¯ä¸ªæ¡ç›®çš„ 'translation' å­—æ®µå¡«å…¥æ‚¨çš„è¯‘æ–‡ï¼Œç„¶åè¿è¡Œ apply-fix å‘½ä»¤",
            "_å‘½ä»¤ç¤ºä¾‹": f"python3 main.py apply-fix --json \"{json_path}\"",
            "generated_at": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "total_issues": total_issues,
            "books": []
        }
        
        for epub_path, details in reports.items():
            book_entry = {
                "epub_file": str(epub_path),
                "epub_name": Path(epub_path).name,
                "segments": []
            }
            
            for idx, item in enumerate(details):
                segment = {
                    "id": idx + 1,
                    "html_file": item['file'],
                    "tag": item['tag'],
                    "original": item.get('full_text', item['text']),  # å®Œæ•´åŸæ–‡
                    "translation": ""  # <-- ç”¨æˆ·åœ¨è¿™é‡Œå¡«å†™è¯‘æ–‡
                }
                book_entry["segments"].append(segment)
            
            json_data["books"].append(book_entry)
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        
        # ========== 3. æ‰“å°æç¤º ==========
        print(f"\n{'='*60}")
        print(f"ğŸ“‹ [é˜¶æ®µ3] æ¼è¯‘æŠ¥å‘Šå·²ç”Ÿæˆ")
        print(f"{'='*60}")
        print(f"   ğŸ“„ å¯è¯»æŠ¥å‘Š: {report_path}")
        print(f"   ğŸ“ äººå·¥ç¿»è¯‘: {json_path}")
        print(f"   æ¶‰åŠ: {len(reports)} æœ¬ä¹¦ï¼Œå…± {total_issues} å¤„æ¼è¯‘")
        print(f"\nğŸ’¡ äººå·¥ç¿»è¯‘æµç¨‹:")
        print(f"   1. æ‰“å¼€ '{json_path.name}'")
        print(f"   2. åœ¨æ¯ä¸ªæ¡ç›®çš„ \"translation\" å­—æ®µå¡«å…¥æ‚¨çš„è¯‘æ–‡")
        print(f"   3. è¿è¡Œ: python3 main.py apply-fix --json \"{json_path}\"")
        print(f"{'='*60}")

    async def _handle_json_command(self, args):
        logger.info(f"å¼€å§‹JSONç¿»è¯‘: {args.file}")
        config = self._get_config(args)
        async with self._create_translator(config) as translator:
            processor = JSONProcessor(translator)
            try:
                result = await processor.translate_file(args.file, args.output, args.source_lang, args.target_lang)
                logger.info(f"JSONç¿»è¯‘å®Œæˆ! è¿›åº¦: {result.get('progress', 0)}%")
                self._print_stats(translator)
            except Exception as e:
                logger.error(f"JSONç¿»è¯‘å¤±è´¥: {e}")
                sys.exit(1)

    async def _handle_html_command(self, args):
        logger.info(f"å¼€å§‹HTMLç¿»è¯‘: {args.file}")
        config = self._get_config(args)
        async with self._create_translator(config) as translator:
            processor = HTMLProcessor(translator)
            try:
                result = await processor.process_file(args.file, args.output, args.source_lang, args.target_lang)
                logger.info(f"HTMLç¿»è¯‘å®Œæˆ! å·²ç¿»è¯‘æ–‡æœ¬å—: {result.get('translated_count', 0)}")
                self._print_stats(translator)
            except Exception as e:
                logger.error(f"HTMLç¿»è¯‘å¤±è´¥: {e}")
                sys.exit(1)

    def _handle_server_command(self, args):
        run_server(host=args.host, port=args.port, api_key=args.api_key, debug=args.debug)

    def _handle_applyfix_command(self, args):
        """è¯»å–äººå·¥ç¿»è¯‘ JSON å¹¶å›å¡«åˆ° ePub"""
        import json
        from bs4 import BeautifulSoup
        
        json_path = Path(args.json)
        if not json_path.exists():
            logger.error(f"æ‰¾ä¸åˆ° JSON æ–‡ä»¶: {json_path}")
            sys.exit(1)
        
        print(f"\nğŸ“– è¯»å–äººå·¥ç¿»è¯‘æ–‡ä»¶: {json_path}")
        
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        books = data.get('books', [])
        if not books:
            print("âš ï¸  JSON ä¸­æ²¡æœ‰éœ€è¦å¤„ç†çš„å†…å®¹")
            return
        
        total_applied = 0
        total_skipped = 0
        
        for book in books:
            epub_path = Path(book['epub_file'])
            if not epub_path.exists():
                logger.warning(f"âš ï¸  è·³è¿‡ (æ–‡ä»¶ä¸å­˜åœ¨): {epub_path}")
                continue
            
            segments = book.get('segments', [])
            # è¿‡æ»¤å‡ºæœ‰è¯‘æ–‡çš„æ¡ç›®
            segments_with_trans = [s for s in segments if s.get('translation', '').strip()]
            
            if not segments_with_trans:
                print(f"â­ï¸  {book['epub_name']}: æ— éœ€å›å¡« (æ²¡æœ‰å¡«å†™è¯‘æ–‡)")
                continue
            
            print(f"\nğŸ“˜ å¤„ç†: {book['epub_name']} ({len(segments_with_trans)} å¤„è¯‘æ–‡)")
            
            # æŒ‰ html_file åˆ†ç»„
            by_file = {}
            for seg in segments_with_trans:
                html_file = seg['html_file']
                if html_file not in by_file:
                    by_file[html_file] = []
                by_file[html_file].append(seg)
            
            # è§£å‹ -> ä¿®æ”¹ -> æ‰“åŒ…
            with tempfile.TemporaryDirectory() as temp_dir:
                with zipfile.ZipFile(epub_path, 'r') as zf:
                    zf.extractall(temp_dir)
                
                for html_file, segs in by_file.items():
                    full_path = os.path.join(temp_dir, html_file)
                    if not os.path.exists(full_path):
                        logger.warning(f"   æ–‡ä»¶ä¸å­˜åœ¨: {html_file}")
                        continue
                    
                    # è¯»å–å¹¶è§£æ HTML
                    with open(full_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    soup = BeautifulSoup(content, 'html.parser')
                    modified = False
                    
                    for seg in segs:
                        original = seg['original']
                        translation = seg['translation']
                        tag_name = seg['tag']
                        
                        # æŸ¥æ‰¾åŒ¹é…çš„å…ƒç´ 
                        for elem in soup.find_all(tag_name):
                            elem_text = elem.get_text(" ", strip=True)
                            # ç²¾ç¡®åŒ¹é…æˆ–åŒ…å«åŒ¹é…
                            if elem_text == original or original in elem_text:
                                # æ›¿æ¢æ–‡æœ¬å†…å®¹
                                elem.clear()
                                elem.append(translation)
                                modified = True
                                total_applied += 1
                                logger.info(f"   âœ… å·²æ›¿æ¢: {original[:30]}... â†’ {translation[:30]}...")
                                break
                        else:
                            total_skipped += 1
                            logger.warning(f"   âš ï¸  æœªæ‰¾åˆ°åŒ¹é…: {original[:50]}...")
                    
                    if modified:
                        with open(full_path, 'w', encoding='utf-8') as f:
                            f.write(str(soup))
                
                # é‡æ–°æ‰“åŒ…
                self._repack_epub(temp_dir, str(epub_path))
                print(f"   âœ… å·²æ›´æ–°: {epub_path}")
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š å›å¡«å®Œæˆ!")
        print(f"   âœ… æˆåŠŸæ›¿æ¢: {total_applied} å¤„")
        print(f"   âš ï¸  æœªæ‰¾åˆ°åŒ¹é…: {total_skipped} å¤„")
        print(f"{'='*60}")

    def _handle_genjson_command(self, args):
        """æ‰«æå·²ç¿»è¯‘çš„ EPUB ç›®å½•ï¼Œç”Ÿæˆäººå·¥ç¿»è¯‘ JSON"""
        import json
        from datetime import datetime
        
        if not EPUBTranslationChecker:
            logger.error("ç¼ºå°‘ check_untranslated.pyï¼Œæ— æ³•æ‰§è¡Œè´¨æ£€")
            sys.exit(1)
        
        target_dir = Path(args.dir)
        if not target_dir.exists():
            logger.error(f"ç›®å½•ä¸å­˜åœ¨: {target_dir}")
            sys.exit(1)
        
        # æŸ¥æ‰¾æ‰€æœ‰å·²ç¿»è¯‘çš„ EPUB
        epub_files = list(target_dir.glob("*_translated.epub"))
        if not epub_files:
            print(f"âš ï¸  åœ¨ {target_dir} ä¸­æ²¡æœ‰æ‰¾åˆ° *_translated.epub æ–‡ä»¶")
            return
        
        print(f"\nğŸ” æ‰«æç›®å½•: {target_dir}")
        print(f"   å‘ç° {len(epub_files)} ä¸ªå·²ç¿»è¯‘ EPUB")
        print("="*60)
        
        checker = EPUBTranslationChecker()
        all_reports = {}
        
        for epub_path in epub_files:
            try:
                report = checker.check_epub(str(epub_path))
                if report['untranslated_count'] > 0:
                    all_reports[str(epub_path)] = report['details']
                    print(f"   âš ï¸  {epub_path.name}: {report['untranslated_count']} å¤„æ¼è¯‘")
                else:
                    print(f"   âœ… {epub_path.name}: æ— æ¼è¯‘")
            except Exception as e:
                logger.warning(f"æ£€æŸ¥ {epub_path.name} å¤±è´¥: {e}")
        
        if not all_reports:
            print("\nğŸ‰ æ‰€æœ‰æ–‡ä»¶å‡æ— æ¼è¯‘ï¼Œæ— éœ€ç”Ÿæˆ JSON")
            return
        
        # è°ƒç”¨ç°æœ‰çš„æŠ¥å‘Šç”Ÿæˆæ–¹æ³•
        self._generate_final_report(all_reports, target_dir)

    def run(self):
        args = self.parser.parse_args()
        if not args.command:
            self.parser.print_help()
            return 0
        if args.verbose:
            logging.getLogger().setLevel(logging.DEBUG)
        
        try:
            if args.command == "json": asyncio.run(self._handle_json_command(args))
            elif args.command == "html": asyncio.run(self._handle_html_command(args))
            elif args.command == "epub": asyncio.run(self._handle_epub_command(args))
            elif args.command == "server": self._handle_server_command(args)
            elif args.command == "apply-fix": self._handle_applyfix_command(args)
            elif args.command == "generate-json": self._handle_genjson_command(args)
        except KeyboardInterrupt:
            print("\nâš ï¸ ä»»åŠ¡è¢«ç”¨æˆ·ä¸­æ–­")
        except Exception as e:
            logger.critical(f"å‘ç”Ÿæœªå¤„ç†çš„å¼‚å¸¸: {e}", exc_info=True)
            return 1
        return 0

if __name__ == "__main__":
    cli = MainCLI()
    sys.exit(cli.run())